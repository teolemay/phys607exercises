{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba24322a-cd91-445c-a0c4-c6bfee7dae86",
   "metadata": {},
   "source": [
    "# Astrostats Week 12: Dimension Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114a9d8-bf67-409d-9613-6da0dd922681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b197d9-01ac-468c-bf8c-306308674aa0",
   "metadata": {},
   "source": [
    "---\n",
    "## Dimensionality and the \"Curse of Dimensionality\"\n",
    "\n",
    "Key points from the lecture:\n",
    "- Each of the “things” we can measure about an object is a “dimension” or “feature” of that object\n",
    "- We need D pieces of information (dimensions) to describe the location of each object in a D-dimensional space\n",
    "- The intrinsic dimension of a space is the minimum number of dimensions you need to describe a location in that space \n",
    "- The extrinsic dimension of a space is the number of dimensions you actually measure\n",
    "\n",
    "- The more dimensions a dataset has, the more data is required to constrain a model\n",
    "\n",
    "### Exercise 1: Investiage the \"Curse of Dimensionality\"\n",
    "\n",
    "Generate $n$ random points in $D$-dimensions and count how many of those points fall within a $D$-dimensional hypersphere of radius $1$ centered on the  origin.  \n",
    "When generating the random points, restrict them to some reasonable range such as $[-2,2]$ in each dimension. \n",
    "\n",
    "How does the number of points within the hypersphere scale with dimension? Plot counts vs dimensions. \n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Hint 1:</summary>\n",
    "For example:   \n",
    "    \n",
    "    - in the 2-D case generate n random points (x,y) and count how many points fall within a circle centered on the origin.  \n",
    "    - in the 3-D case generate n random points (x,y,z) and count how many points fall within a sphere centered on the origin.  \n",
    "Extrapolate this for higher dimensions. \n",
    "</details>\n",
    "  \n",
    "\n",
    "<details>\n",
    "  <summary>Hint 2:</summary>\n",
    "Points fall within the hypersphere of radius $1$ if their Euclidean distance from the origin is less than or equal to 1. \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Hint 3:</summary>\n",
    "The D-dimensionsal Euclidean distance to the origin is given by:\n",
    "\\begin{equation}\n",
    "d = \\sqrt{ \\sum_{i=0}^{D} ( x_i )^2 }\n",
    "\\end{equation}\n",
    "Check if this distance is less than one for each point. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7f005-42cd-49fe-bd7a-442b5526e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countPointsinHyperSphere( D, n = int(1e3) ): \n",
    "    '''\n",
    "    Count the number of randomly generated points which fall inside a D-dimensional hypersphere\n",
    "\n",
    "    Inputs:\n",
    "    D = number of dimensions \n",
    "    n = number of points\n",
    "\n",
    "    Returns:\n",
    "    cnt = number of points which fell inside the D-dimensional hypersphere\n",
    "    '''\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "\n",
    "    return cnt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Number of points within a hypersphere as a function of dimension')\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Number of points')\n",
    "\n",
    "plt.plot( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90aace",
   "metadata": {},
   "source": [
    "---\n",
    "## Principal Component Analysis, Principal Component Plane and reconstruction of data\n",
    "\n",
    "\n",
    "\n",
    "We will now build our own function to perform PCA usind singular value decomposition (SVD).\n",
    "Then we will use this function to work with SDSS galaxy spectral. data.\n",
    "\n",
    "\n",
    "### Exercise: Build and play with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6ec09-f0bb-40b4-857e-82479de700a3",
   "metadata": {},
   "source": [
    "#### Step 1: Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae013f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''First, let`s install python module to get SDSS data'''\n",
    "!pip install astroML\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Let`s now fetch spectra of 4000 galaxies that span across 1000 wavelength bins'''\n",
    "\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra() # data dictionary\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data) # [4000, 1000] data\n",
    "wavelength = sdss_corrected_spectra.compute_wavelengths(data) # wavelength for spectra\n",
    "cl = data['spec_cln'] # spectral class (yes, not everything is galaxy spectra here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecdd3d3",
   "metadata": {},
   "source": [
    "#### Step 2: Standardize the Data  \n",
    "Now when we have our data, before we run PCA on it, we have to rescale it.   \n",
    "If our data had features with different units, we should standardize features by removing the mean and scaling to unit variance \n",
    "(see [StandardScaler - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)). \n",
    "\n",
    "In this exercise, the features are all flux at different wavelengths. We can standardize the data by subtracting the mean spectrum from each individual spectrum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db773359",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate average spectra and scale the data ###\n",
    "\n",
    "rescaled_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865b321",
   "metadata": {},
   "source": [
    "#### Step 3: Write your PCA Function and Perform PCA\n",
    "Use singular value decomposition (SVD) to write function for PCA that will take our data and output eigenvectors, eigenvalues and projection of our data on principal components. \n",
    "\n",
    "You can use the numpy function for SVD ([numpy.linalg.svd - NumPy](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)).\n",
    "\n",
    "\n",
    "<details>\n",
    "        <summary>Hint</summary>\n",
    "    \n",
    "    U, Sigma, VT = np.linalg.svd(rescaled_data, full_matrices=False)\n",
    "    \n",
    "Sigma is an array of sqrt(eigenvalues)\n",
    "VT is an eigenvector matrix\n",
    "    \n",
    "Projection is just a dot product of data and eigenvectors\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(rescaled_data):\n",
    "    '''\n",
    "    Principal Components Analysis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rescaled_data : Array [M, F]\n",
    "        M - number of samples of data. F - number of features.\n",
    "        In your case it`s [galaxies, wavelengthes]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ei_vals : Array [min(M, F),]\n",
    "        Eigenvalues which define the amount of variance contained within each component.\n",
    "    ei_vecs : Array [min(M, F), min(M, F)]\n",
    "        Eigenvectors (principal components) - vectors that are aligned with the deriction of maximal variance.\n",
    "    projected : Array [M, min(M, F)]\n",
    "        Data projected on eigenvectors.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    return ei_vals, ei_vecs, projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f30c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_vals, ei_vecs, projected = pca(rescaled_data)\n",
    "\n",
    "total_variance = \n",
    "variance_ratio = \n",
    "cumulative_variance_ratio = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a1e78-a05c-48e7-bd07-4b014764636e",
   "metadata": {},
   "source": [
    "#### Step 4:  Make a Scree Plot\n",
    "\n",
    "\n",
    "Then build a plot that can tell us how many principal components are enought to characterize majority of variane in our data.\n",
    "\n",
    "<details>\n",
    "        <summary>Hint</summary>\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7.5))\n",
    "    fig.subplots_adjust(hspace=0.05, bottom=0.12)\n",
    "\n",
    "    ax = fig.add_subplot(211, xscale='log', yscale='log')\n",
    "    ax.grid()\n",
    "    ax.plot(variance_ratio, c='k')\n",
    "    ax.set_ylabel('Normalized Eigenvalues')\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax = fig.add_subplot(212)\n",
    "    ax.grid()\n",
    "    ax.semilogx(cumulative_variance_ratio, color='k')\n",
    "    ax.set_xlabel('Eigenvalue Number')\n",
    "    ax.set_ylabel('Cumulative Eigenvalues')\n",
    "    plt.show()\n",
    "  \n",
    "OR\n",
    "  \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, len(ei_vals) + 1), variance_ratio, alpha=0.7, align='center', label='Individual explained variance')\n",
    "    plt.step(range(1, len(ei_vals) + 1), cumulative_variance_ratio, where='mid', label='Cumulative explained variance')\n",
    "    plt.xlabel('Principal Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(1,1000)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00368204-45ca-479d-8283-d079f757db61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f687face",
   "metadata": {},
   "source": [
    "#### Step 5: Plot the Projected Data in the Principal Component Plane\n",
    "\n",
    "It is often usefull in the analysis to plot Principal Component Plane (PCP). It's even better when the data can be mostly described by 2 Principal Components, which is our case. Make a scatter plot using any 2 projected data vectors.\n",
    "\n",
    "\n",
    "Try making this plot for many different combinations of principal components.  That is, other projections of the data. \n",
    "<details>\n",
    "        <summary>Hint</summary>\n",
    "    \n",
    "    plt.scatter(projected[:,PC_index_1], projected[:,PC_index_2], s=1, zorder=1)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ff809",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_index_1 = \n",
    "PC_index_2 = \n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.xlabel('Principal Component {}'.format(PC_index_1))\n",
    "plt.ylabel('Principal Component {}'.format(PC_index_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a1730-cdbf-4970-a638-d0e1f27bb8ae",
   "metadata": {},
   "source": [
    "#### Step 6:  Identify outliers, extreme cases, and/or cluster the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249c04f",
   "metadata": {},
   "source": [
    "PCP can help to identify outliers, extreme data cases and cluster the data. Our spectra is not completely consists of galaxies. Add these lines to your plot and see where non-galactic spectra lies on PCP. You can also play arround and plot other 2 projections.\n",
    "```\n",
    "star_mask = (cl == 1)\n",
    "quasar_mask = (cl == 3) | (cl == 4)\n",
    "redstar_mask = (cl == 6)\n",
    "unknown_mask = (cl == 0)\n",
    "```\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    \n",
    "    plt.scatter(projected[:,PC_index_1][unknown_mask], \n",
    "            projected[:,PC_index_2][unknown_mask], s=1, c='black',zorder=10, label='undefined')\n",
    "    plt.scatter(projected[:,PC_index_1][quasar_mask], \n",
    "            projected[:,PC_index_2][quasar_mask], s=1, c='orange',zorder=10, label='quasars')\n",
    "    plt.scatter(projected[:,PC_index_1][redstar_mask], \n",
    "            projected[:,PC_index_2][redstar_mask], s=1, c='red',zorder=10, label='late-type stars')\n",
    "    plt.scatter(projected[:,PC_index_1][star_mask], \n",
    "            projected[:,PC_index_2][star_mask], s=1, c='purple',zorder=10, label='stars')\n",
    "    \n",
    "<details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71664d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_mask = (cl == 1)\n",
    "quasar_mask = (cl == 3) | (cl == 4)\n",
    "redstar_mask = (cl == 6)\n",
    "unknown_mask = (cl == 0)\n",
    "\n",
    "PC_index_1 = \n",
    "PC_index_2 = \n",
    "\n",
    "\n",
    "### Plot here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84f162",
   "metadata": {},
   "source": [
    "#### Step 7:  Reconstruct the Data\n",
    "\n",
    "We can reconstruct data points from PC-projection back to real data. This is used to see how outliers look, explore extreme cases or reduce noise of the data.\n",
    "\n",
    "Build a function to deproject data from PC hyperplane, rescale and retrieved real data. It is easy to see that zero-vector on PC hyperplane will be equal to the mean data after deprojection. \n",
    "\n",
    "Try it for yourself. If deprojected zero-vector is = to mean spectra, this means your PCA is correct. You can then deproject different coordinates or whole data and see what it looks like.\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    \n",
    "    reconstructed = np.dot(PC_coords, eigenvectors)\n",
    "\n",
    "    reconstructed = reconstructed + mean\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(PC_coords, eigenvectors, mean):\n",
    "    '''\n",
    "    Reconstructs data from projection coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    PCP_coords : Array [M , K]\n",
    "        Coodinates in PC hyperplane.\n",
    "        M is number of measurements, K is number of principal components.\n",
    "        Example: if you want to reconstruct single point on 2D PCP, \n",
    "        your PCP_coords are gonna be [[x, y]].\n",
    "    eigenvectors : Array [K , F]\n",
    "        Eigenvector array of your PCA.\n",
    "        K is number of principal components. F is number of features, in your case –\n",
    "        number of wavelength.\n",
    "    mean : array [M,]\n",
    "        Array of average values of your raw data. In your case – average spectra\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reconstructed : Array [M x F]\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    return reconstructed\n",
    "    \n",
    "\n",
    "### Plot reconstructed data point from PCP coordinates ###\n",
    "\n",
    "n_comp =  #number of PCs used to reconstruct data\n",
    "\n",
    "### Plot reconstructed data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f483ef",
   "metadata": {},
   "source": [
    "#### Step 8: One last thing. \n

    "We can reconstruct our data into lower dimensions by trimming our eigenvectors. This can help in further modeling and analysis.\n",
    "\n",
    "Write a code to reconstruct data onto less number of wavelengths (you can create a random mask for eigenvectors):\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    \n",
    "    interesting_wls = np.sort(np.random.randint(0, 1000, size=number_of_wavelenghths))[::-1]\n",
    "\n",
    "    reconstructed = reconstruct(projected[:, :n_comp], \n",
    "                            ei_vecs[:n_comp, interesting_wls], \n",
    "                            avg_spectra[interesting_wls])\n",
    "\n",
    "\n",
    "    plt.plot(wavelength[interesting_wls], reconstructed.T, linewidth=0.2, alpha=0.5)\n",
    "    plt.title('reconstructed data; '+ str(n_comp)+' PCs')\n",
    "    plt.show()\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = \n",
    "\n",
    "### Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0b957-2c6e-4d3c-9fcf-1626e5a8854a",
   "metadata": {},
   "source": [
    "#### sklearn PCA\n",
    "\n",
    "Most common way to do PCA in python is to use [sklearn.decomposition.PCA - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "\n",
    "Try to repeat steps we covered above and see if there is a difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42730abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(rescaled_data)\n",
    "projected = pca.transform(rescaled_data)\n",
    "ei_vals = pca.explained_variance_\n",
    "ei_vecs = pca.components_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
